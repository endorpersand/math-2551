# 14.9: Taylor's Formula for $\large f(x, y)$
**Taylor Polynomial** (recap)
If function $f$ has $n$ derivatives at point where $x = a$, then the nth Taylor Polynomial for $f$ at $a$ is:
$$
\Large
P_n(x) = \sum\limits^n_{k = 0}{\frac{f^{(k)}(a)(x-a)^k}{k!}}
$$
**The theorem**
If $f$ has $n + 1$ derivatives on an open interval containing $a$, then for every $x$ in that open interval, we have:
$$
\Large
f(x)=P_n(x) + \underbrace{\frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1}}_{\text{error term}}
$$
for some (estimated) value $c$ between $a$ and $x$ that maximizes that term.

The absolute value of last term is called the error when using $P_n(x)$ to approximate $f(x)$.
$$
\Large
\text{error} = |f(x) - P_n(x)| = \frac{|f^{(n+1)}(c)|}{(n+1)!}|x-a|^{n+1}
$$
![[Pasted image 20220119184824.png]]
(I believe this was done in BC)

## Two Variables
Suppose $f(x, y)$ and its partials through order $n+1$ are continuous throughout open rectangular region $R$ centered around $(a, b)$. 
Then, throughout R:
$$
\Large

f(a + h, b + k) = \sum\limits_{i=0}^{n}{\left.\frac{1}{i!}\left(h\frac{\partial}{\partial x} + k\frac{\partial}{\partial y}\right)^{i}f\right|_{(a,b)}} + E(a+h, b+k)
$$
The last term $E(a+h, b+k)$ is the error term. It is evaluated at the point on the line segment connecting $(a, b)$ and $(a + h, b + k)$ that maximizes the error term.

This error term is defined as:
$$
\Large
E(a+h, b+k) = \left.\frac{1}{(n+1)!}\left(h\frac{\partial}{\partial x} + k\frac{\partial}{\partial y}\right)^{(n+1)}f\right|_{(a+ch,b+ck)}
$$

**Expanded form**
$$
\Large
\begin{align*}
f(a + h, b + k) &= f(a,b) + (hf_x + kf_y)|_{(a, b)}\\
&+ \frac{1}{2!}(h^2f_{xx} + 2hkf_{xy} + k^2f_{yy})|_{(a, b)}\\
&+ \frac{1}{3!}(h^3f_{xxx} + 3h^2kf_{xxy} + 3hk^2f_{xyy} + k^3f_{yyy})|_{(a, b)}\\
&+ \cdots\\
&+ \frac{1}{n!}\left.\left(h \frac{\partial}{\partial{x}} + k \frac{\partial}{\partial{y}}\right)^n\right|_{(a, b)}
\end{align*}
$$

**Wait, isn't the two-variable Taylor's formula an extension of single-variable Taylor's? Where's the $(x-a)$ term?**

Note that in the single-variable Taylor's formula, $x$ represents... just $x$.
However, two-variable Taylor's does not use $x$. It defines $h$ and $k$, which represent a RELATIVE DISTANCE from $a$ and $b$.

You can also write single-variable Taylor's in the form of the evaluation point $x = a$ and a relative distance $h$ from that evaluation point (i.e. let $x = a + h$):
$$
\Large
f(a + h)= \sum\limits^n_{k = 0}{\frac{h^kf^{(k)}(a)}{k!}} + \underbrace{\frac{f^{(n+1)}(c)}{(n+1)!}h^{n+1}}_{\text{error term}}
$$
This aligns much more closely to the two-variable formula.

# 14.10: Partial Derivatives w/ Constraints
Sometimes, some of the variables are dependent on others.
The notation $\Large \left(\frac{\partial{w}}{\partial{y}}\right)_{z, t}$ represents the partial derivative of $w$ with respect to $y$, given that $z$ and $t$ are independent.

To evaluate a partial derivative with constraints:
1. Decide which variables are dependent & independent
2. Eliminate the other dependent variables
3. Differentiate and solve

## Example
If $\Large w = x^2 + y - z + \sin(t)$ and $\Large x + y = t$, find $\Large \left(\frac{\partial{w}}{\partial{y}}\right)_{z, t}$

#### Method 1: Eliminating other independent variables first.
Since $x + y = t$, substitute $x = t - y$ into $w$, resulting in:
$$
\Large
w = (t - y)^2 + y - z + \sin(t)
$$
All of the variables are now independent, so just compute the partial.
$$
\Large
\frac{\partial{w}}{\partial{y}} = -2(t-y) + 1
$$

#### Method 2: Deriving on the go.
We know $x$ is dependent on $y$, so $\frac{\partial{x}}{\partial{y}}$ must be non-zero.
$$
\Large
\frac{\partial{w}}{\partial{y}} = 2x \frac{\partial{x}}{\partial{y}} + 1
$$

Then, since $x = t - y$,
$$
\Large
\frac{\partial{x}}{\partial{y}} = -1
$$
Substitute $\frac{\partial{x}}{\partial{y}}$ and $x$, you get the answer from before.

#module2 #week6